{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_02.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EHn8xRsUtmhT"},"source":["# Assignment 2"]},{"cell_type":"markdown","metadata":{"id":"kqvOFR7HvBLm"},"source":["## Problem 1 TF-IDF\n","\n","Implement TF-IDF using using Python, Numpy, Pandas and whatever text cleaning library required.\n","\n","The tfâ€“idf is the product of two statistics, term frequency and inverse document frequency. There are various ways for determining the exact values of both statistics, you can use the following formulas.\n","\n","### Term Frequency\n","$$tf_{t,d} = \\log_{10}(count(t,d) +1)$$ \n","\n","* $tf_{t,d}$ is the frequency of the word t in the\n","document d\n","\n","### Inverse Document Frequency\n","$$idf_t = \\log_{10}(\\frac{N}{df_t})$$\n","\n","* $N$ is the total number of documents\n","* $df_t $ is the number of documents in which term t occurs\n","\n","### TF-IDF\n","$$tf\\text{-}idf_{t,d} = tf_{t,d} \\times idf_t $$\n","\n","### What is expected? \n","Your implementation should include the following two functions:\n"," * `compute_tfidf_weights(train_docs)`\n"," * `word_tfidf_vector(word, tf_df, idf_df)`\n","\n","To revise what TF-IDf is, you can revise the lecture notes and the further reading under Week 7.\n"]},{"cell_type":"code","metadata":{"id":"sVZYldjbXFYt"},"source":["import nltk\n","nltk.download('all')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zS2YXV_ciVHJ"},"source":["import math\n","import pandas as pd\n","import numpy as np\n","import re\n","from nltk.corpus import stopwords\n","set(stopwords.words('english'))\n","\n","def compute_tfidf_weights(train_docs):\n","  '''\n","  Input arguments:\n","    train_docs : list of documents strings\n","  Output arguments:\n","    docs_tf : tf as a DataFrame\n","    docs_idf : idf as a DataFrame\n","  '''\n","  docs_idf ={}\n","  docs_tf = {}\n","  wordfreq = {}\n","  for train in train_docs:\n","    corpus = nltk.sent_tokenize(train)\n","\n","    for i in range(len(corpus )):\n","        corpus [i] = corpus [i].lower()\n","        corpus [i] = re.sub(r'\\W',' ',corpus [i])\n","        corpus [i] = re.sub(r'\\s+',' ',corpus [i])\n","        \n","\n","  \n","    for sentence in corpus:\n","        tokens = nltk.word_tokenize(sentence)\n","        for token in tokens:\n","            if token not in wordfreq.keys():\n","                wordfreq[token] = 1\n","            else:\n","                wordfreq[token] += 1\n","  \n","  \n"," \n","  for word, count in wordfreq.items():\n","\n","    docs_tf[word] =  math.log10(count + 1 )\n","    \n","  N = len(wordfreq)\n","\n","  for word, val in wordfreq.items():\n","      docs_idf[word] = math.log10(N / float(val) + 1)\n","  \n","  \n","  data_tf= list(docs_tf.items())\n","  data_idf= list(docs_idf.items())\n"," \n","  df_tf = pd.DataFrame(data_tf)\n","  df_idf = pd.DataFrame(data_idf)\n","  \n","  return df_tf , df_idf\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gm2GQtRjXpNE","executionInfo":{"status":"ok","timestamp":1621761804417,"user_tz":-600,"elapsed":268,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}},"outputId":"32b9238a-3915-4a80-f5e2-7c2c5e3e09b3"},"source":[" lst_1=  ['Geeksforgeeks is a portal for geeks','hello how are you','hello am good how about yourself','am doing how are you agagian']\n","compute_tfidf_weights(lst_1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(                0         1\n"," 0   geeksforgeeks  0.301030\n"," 1              is  0.301030\n"," 2               a  0.301030\n"," 3          portal  0.301030\n"," 4             for  0.301030\n"," 5           geeks  0.301030\n"," 6           hello  0.477121\n"," 7             how  0.602060\n"," 8             are  0.477121\n"," 9             you  0.477121\n"," 10             am  0.477121\n"," 11           good  0.301030\n"," 12          about  0.301030\n"," 13       yourself  0.301030\n"," 14          doing  0.301030\n"," 15        agagian  0.301030,                 0         1\n"," 0   geeksforgeeks  1.230449\n"," 1              is  1.230449\n"," 2               a  1.230449\n"," 3          portal  1.230449\n"," 4             for  1.230449\n"," 5           geeks  1.230449\n"," 6           hello  0.954243\n"," 7             how  0.801632\n"," 8             are  0.954243\n"," 9             you  0.954243\n"," 10             am  0.954243\n"," 11           good  1.230449\n"," 12          about  1.230449\n"," 13       yourself  1.230449\n"," 14          doing  1.230449\n"," 15        agagian  1.230449)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"SBSQ3QmWg2gh"},"source":["def word_tfidf_vector(word, tf_df, idf_df):\n","  '''\n","    Input arguments:\n","      word : a query string\n","      tf_tf : tf as a DataFrame\n","      tf_idf : idf as a DataFrame\n","    Output arguments:\n","      tf_idf_value : a numpy array of dimension 1xN\n","  '''\n","  tf_df = dict(tf_df.values)\n","  idf_df = dict(idf_df.values)\n","  corpus = nltk.sent_tokenize(word)\n","\n","  for i in range(len(corpus )):\n","      corpus [i] = corpus [i].lower()\n","      corpus [i] = re.sub(r'\\W',' ',corpus [i])\n","      corpus [i] = re.sub(r'\\s+',' ',corpus [i])\n","\n","  tfidf_values = {}\n","  for sentence in corpus:\n","      tokens = nltk.word_tokenize(sentence)\n","  for token in tokens:\n","    if token in tf_df.keys() and token in idf_df.keys():\n","        for val in tf_df.values():\n","          tfidf_values[token] = val * idf_df[token]\n","      \n","    \n","  tf_idf_model = np.asarray(tfidf_values)\n","  return tf_idf_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pc38q_SUcxoV","executionInfo":{"status":"ok","timestamp":1621761846362,"user_tz":-600,"elapsed":454,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}},"outputId":"d1a6dd77-04b2-44a4-ae0f-b991f6bf4649"},"source":["lst_1=  ['Geeksforgeeks is a portal for geeks','hello how are you','hello am good how about yourself','am doing how are you again']\n","lst =  ' about yourself am doing how are you agagin doing gretaeakjsdkjahsdjkasd kjashdkjashdkbkashbd asjahdjka'\n","tdf,idf = compute_tfidf_weights(lst_1)\n","word_tfidf_vector(lst,tdf,idf)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array({'about': 0.37040203346725215, 'yourself': 0.37040203346725215, 'am': 0.2872556184789065, 'doing': 0.37040203346725215, 'how': 0.24131538171067718, 'are': 0.2872556184789065, 'you': 0.2872556184789065},\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"JN4SjrEYuFOx"},"source":["## Problem 2 Word embedding as features for classification\n","\n","### Task\n","Implement legal case type classification on a legal case corpus. The corpus contains 39,155 legal cases including 22,776 taken from the United States supreme court.\n","\n","You can find more details about the dataset at https://osf.io/qvg8s/wiki/home/ \n","\n","Implement classification using necessary libraries with the features being GloVe word embeddings using Gensim as demonstrated below. \n","\n","Report the accuracy and F1 score (micro- and macro-averaged).\n","\n","### Dataset\n","The dataset can downloaded at https://osf.io/qvg8s/files/.\n","The files of interest are as highlighed below:\n","\n","<center>\n","<img width=\"900px\" src=\"https://drive.google.com/uc?id=1RUVQ8rGyjrv2gspluJM5f6lbqBS6k4yJ\"> \n","</center>\n","\n","\n","### Document representation\n","Convert words after cleaning into their embeddings, then take the average of of all the words in a case (document) to end up with single vector representing each case. the case vector is then used for case classification.\n","\n","In the process of finding the embeddings for each word, you can ignore out-of-vocabulary words.\n","\n","### Classifier choice\n","The choice of classification method(s) is left to you. You are expected to experiment with more that one type of classifier and comment on your findings.\n","\n","### Suggestion (Optional)\n","Consider saving a cleaned up version of the dataset after creating the embeddings to a file which can be loaded and used for further experimentation. "]},{"cell_type":"code","metadata":{"id":"Sq_Vs0coAuiX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622029358625,"user_tz":-600,"elapsed":25955,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}},"outputId":"f2997b9a-854e-4da2-882c-94a27c26da21"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":164,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VB5jDv_mAaro","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622029667586,"user_tz":-600,"elapsed":14808,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}},"outputId":"c28437cd-d42e-4bed-afda-b673f07b8cba"},"source":["%%shell\n","DATA_URL='https://osf.io/8mjcy/download'\n","\n","pushd /content\n","wget $DATA_URL -O data.zip\n","unzip -q data.zip\n","popd"],"execution_count":171,"outputs":[{"output_type":"stream","text":["/content /content\n","--2021-05-26 11:47:33--  https://osf.io/8mjcy/download\n","Resolving osf.io (osf.io)... 35.190.84.173\n","Connecting to osf.io (osf.io)|35.190.84.173|:443... connected.\n","HTTP request sent, awaiting response... 302 FOUND\n","Location: https://files.osf.io/v1/resources/qvg8s/providers/osfstorage/5c2c13130e8efd0017d0aa06?action=download&direct&version=1 [following]\n","--2021-05-26 11:47:33--  https://files.osf.io/v1/resources/qvg8s/providers/osfstorage/5c2c13130e8efd0017d0aa06?action=download&direct&version=1\n","Resolving files.osf.io (files.osf.io)... 35.186.214.196\n","Connecting to files.osf.io (files.osf.io)|35.186.214.196|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 228944703 (218M) [application/octet-stream]\n","Saving to: â€˜data.zipâ€™\n","\n","data.zip            100%[===================>] 218.34M  79.8MB/s    in 2.7s    \n","\n","2021-05-26 11:47:37 (79.8 MB/s) - â€˜data.zipâ€™ saved [228944703/228944703]\n","\n","/content\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{"tags":[]},"execution_count":171}]},{"cell_type":"code","metadata":{"id":"TW3GIockAal4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622030944206,"user_tz":-600,"elapsed":64132,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}},"outputId":"d964a765-b2c1-4bf6-ba33-051038b3bd5d"},"source":["import nltk \n","nltk.download('all')"],"execution_count":190,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":190}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wp6BvFDzPWb-","executionInfo":{"status":"ok","timestamp":1622030302966,"user_tz":-600,"elapsed":448,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}},"outputId":"0f326a5a-16e8-4aea-b0d2-41a8269bc824"},"source":["import gensim.downloader as api\n","import pandas as pd\n","import os\n","import glob\n","list(api.info()['models'].keys())"],"execution_count":185,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['fasttext-wiki-news-subwords-300',\n"," 'conceptnet-numberbatch-17-06-300',\n"," 'word2vec-ruscorpora-300',\n"," 'word2vec-google-news-300',\n"," 'glove-wiki-gigaword-50',\n"," 'glove-wiki-gigaword-100',\n"," 'glove-wiki-gigaword-200',\n"," 'glove-wiki-gigaword-300',\n"," 'glove-twitter-25',\n"," 'glove-twitter-50',\n"," 'glove-twitter-100',\n"," 'glove-twitter-200',\n"," '__testing_word2vec-matrix-synopsis']"]},"metadata":{"tags":[]},"execution_count":185}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0yOQhQvQGcO","executionInfo":{"status":"ok","timestamp":1622030360295,"user_tz":-600,"elapsed":53362,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}},"outputId":"62eda7c3-767f-4a46-cf3a-19db79b4c6bf"},"source":["model = api.load(\"glove-wiki-gigaword-50\")\n","model.most_similar(\"pneumatic\")"],"execution_count":186,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 66.0/66.0MB downloaded\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[('hydraulic', 0.8155338168144226),\n"," ('actuators', 0.7667093276977539),\n"," ('sprinkler', 0.7374148368835449),\n"," ('valve', 0.727166473865509),\n"," ('actuation', 0.7141326069831848),\n"," ('hose', 0.7138993144035339),\n"," ('paddles', 0.7132105827331543),\n"," ('valves', 0.709661066532135),\n"," ('high-pressure', 0.7025710344314575),\n"," ('turntable', 0.7003635168075562)]"]},"metadata":{"tags":[]},"execution_count":186}]},{"cell_type":"code","metadata":{"id":"1EKAYX2nrXtD","executionInfo":{"status":"ok","timestamp":1622029899500,"user_tz":-600,"elapsed":252,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}}},"source":["import pandas as pd\n","import os\n","# rootdir = \"/content/preprocessed_cases[cases_29404]\"\n","# newrootdir = '/content/preprocessed_cases'\n","# #os.rename(rootdir, newrootdir) #Rename the directory, run only once\n","# x=[]\n","# y=[]\n","# for subdir, dirs, files in os.walk(newrootdir):\n","#   for file in files:\n","#     with open(os.path.join(subdir, file), 'r') as f:\n","#       x.append(f.read())\n","#       y.append(subdir.strip('/content/preprocessed_cases/'))"],"execution_count":173,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_O_yHu-ZeUB","executionInfo":{"status":"ok","timestamp":1622029449725,"user_tz":-600,"elapsed":262,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}}},"source":["import pickle\n","\n","unique_cases_file = '/content/drive/MyDrive/CSE5NLP/unique_cases.zip (Unzipped Files)/unique_cases_dict.pickle'\n","\n","with open(unique_cases_file, 'rb') as handle:\n","    unique_cases_folders = pickle.load(handle)"],"execution_count":166,"outputs":[]},{"cell_type":"code","metadata":{"id":"NuH1WFNSfzlB","executionInfo":{"status":"ok","timestamp":1622030101215,"user_tz":-600,"elapsed":259,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}}},"source":["text=[]\n","casetype=[]\n","for files in unique_cases_folders.keys():\n","  for file in unique_cases_folders[files]:\n","    with open(os.path.join(\"/content/preprocessed_cases[cases_29404]/\"+files, file), 'r') as f:\n","      text.append(f.read())\n","      casetype.append(files)"],"execution_count":178,"outputs":[]},{"cell_type":"code","metadata":{"id":"DJTt05gfhTlj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z53XXIRpb7Yj","executionInfo":{"status":"ok","timestamp":1622029990471,"user_tz":-600,"elapsed":257,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}}},"source":[""],"execution_count":177,"outputs":[]},{"cell_type":"code","metadata":{"id":"IFgrUcFhZ3ki","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1622030154221,"user_tz":-600,"elapsed":259,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}},"outputId":"7f4aad97-439f-4bb9-fe5e-049e91110b2a"},"source":["df = pd.DataFrame({'CaseType': casetype,'sentence':text})\n","df.head()\n"],"execution_count":181,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CaseType</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>41</td>\n","      <td>this case is the second appeal to this court i...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>41</td>\n","      <td>on the evening of saturday,april,hardy haceesa...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>41</td>\n","      <td>brockton hospital petitions for review of a de...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>41</td>\n","      <td>in this action under the individuals with disa...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41</td>\n","      <td>this case involves an issue that has repeatedl...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  CaseType                                           sentence\n","0       41  this case is the second appeal to this court i...\n","1       41  on the evening of saturday,april,hardy haceesa...\n","2       41  brockton hospital petitions for review of a de...\n","3       41  in this action under the individuals with disa...\n","4       41  this case involves an issue that has repeatedl..."]},"metadata":{"tags":[]},"execution_count":181}]},{"cell_type":"code","metadata":{"id":"wa3KPEBfjMAj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RN93tzGhi-SD","executionInfo":{"status":"ok","timestamp":1622031838274,"user_tz":-600,"elapsed":14546,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}}},"source":["import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","stop_words = set(stopwords.words('english'))\n","lemma = WordNetLemmatizer()\n","\n","def clean(text):\n","  text = re.sub(r'http\\S+','',text)\n","  text = re.sub('[^a-zA-Z]',' ',text)\n","  text = str(text).lower()\n","  text = word_tokenize(text)\n","  text = [item for item in text if item not in stop_words]\n","  text = [lemma.lemmatize(word=w,pos='v') for w in text]\n","  text = [i for i in text if len(i)>2]\n","  text = ' '.join(text)\n","  return text \n","\n","df['CleanSentence'] = df['sentence'].apply(clean)\n"],"execution_count":195,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"vnUsp0ZPocw6","executionInfo":{"status":"ok","timestamp":1622031851857,"user_tz":-600,"elapsed":254,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}},"outputId":"ee2f2a5b-6909-47e7-9a13-a407de057fdb"},"source":["df.head()"],"execution_count":196,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CaseType</th>\n","      <th>sentence</th>\n","      <th>CleanSentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>41</td>\n","      <td>this case is the second appeal to this court i...</td>\n","      <td>case second appeal court patent litigation cro...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>41</td>\n","      <td>on the evening of saturday,april,hardy haceesa...</td>\n","      <td>even saturday april hardy haceesa walk hospita...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>41</td>\n","      <td>brockton hospital petitions for review of a de...</td>\n","      <td>brockton hospital petition review decision ord...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>41</td>\n","      <td>in this action under the individuals with disa...</td>\n","      <td>action individuals disabilities education act ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41</td>\n","      <td>this case involves an issue that has repeatedl...</td>\n","      <td>case involve issue repeatedly come federal cou...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  CaseType  ...                                      CleanSentence\n","0       41  ...  case second appeal court patent litigation cro...\n","1       41  ...  even saturday april hardy haceesa walk hospita...\n","2       41  ...  brockton hospital petition review decision ord...\n","3       41  ...  action individuals disabilities education act ...\n","4       41  ...  case involve issue repeatedly come federal cou...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":196}]},{"cell_type":"code","metadata":{"id":"yQBM-2z6oh95","executionInfo":{"status":"ok","timestamp":1622033404576,"user_tz":-600,"elapsed":14453,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}}},"source":["from gensim.models import Word2Vec\n","model = Word2Vec(df['CleanSentence'], min_count=1)"],"execution_count":211,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cs5XDT4P-3py"},"source":["## Problem 3 POS for classification"]},{"cell_type":"markdown","metadata":{"id":"kXb4ZozOmFa_"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"eyQIaUWB6tfR"},"source":["Robots and chat bots receive different commands to do certain tasks. \n","\n","Write a simple pragram that receive interactions in the form of a sentence and return:\n","* A tuple of (command, object) if the sentence is a command\n","* None if the sentence is not a command\n","\n","To write this function, you can utilize a Part-of-speech tagger or named-entity recognizer from libraries like NLTK and Spacy.\n","\n","Consider the following EXAMPLE sentences:\n","\n","* Commands:\n","  * Grab the book\n","  * Fetch the ball\n","  * Open the jar\n","  * Can hand this spoon to John?\n","\n","* Not commands:\n","  * Hey, how is it going?\n","  * How is your day today?\n","  * Do you like the weather?\n","This list is not exhaustive, your function should be able to handle more cases. \n","\n","### Expected outcome:\n","1. A function that performs the task\n","2. If your function has limitations, highlight those limitations with examples."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316},"id":"R12dKka4qUck","executionInfo":{"status":"ok","timestamp":1622028771338,"user_tz":-600,"elapsed":1391,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}},"outputId":"a223bac0-3676-4dcc-9f2d-071114488074"},"source":["import en_core_web_sm\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from spacy import displacy\n","\n","# load en_core_web_sm of English for vocabluary, syntax & entities\n","nlp = en_core_web_sm.load()\n","strn = \" \"\n","filtered_sent={}\n","def command(data):\n","  doc = nlp(data)\n","  displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n","  for word in doc:\n","    if word.pos_ ==\"NOUN\":\n","      if word.dep_==\"dobj\":\n","        # print(word.head.pos_)\n","        # print(\"inside dep\")\n","        if word.head.pos_==\"DET\":\n","          # print(\"2\")\n","          return (doc,word)\n","        elif word.head.pos_==\"VERB\":\n","          strn = [child for child in word.children]\n","          # print(strn)\n","          string =','.join(str(v) for v in strn)\n","          # print(string)\n","          dc = nlp(string)\n","          for i in dc:\n","            print(i.pos_)\n","            if i.pos_ == \"DET\":\n","              return (doc,word)\n","            else:\n","              return None\n","   \n","    elif word.pos_==\"ADV\":\n","      if word.head.pos_==\"PROPN\" or word.head.pos_==\"VERB\":\n","        return (doc,word)\n","    \n","lis = [\"Grab the book\",\"Fetch the ball\",\"Open the jar\",\"Can hand this spoon to John?\"]\n","lis2 =[ \"Hey, how is it going?\",\"How is your day today?\",\"Do you like the weather? This list is not exhaustive, your function should be able to handle more cases.\"]\n","lis3 = [\"Hey, how is it going?\",\"How is your day today?\"]\n","print(command(\"Do you like the weather?\"))\n"],"execution_count":163,"outputs":[{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7dbb82000df8427d8c18d4692c97a364-0\" class=\"displacy\" width=\"500\" height=\"227.0\" direction=\"ltr\" style=\"max-width: none; height: 227.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Do</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">AUX</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">you</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PRON</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">like</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">the</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">weather?</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-7dbb82000df8427d8c18d4692c97a364-0-0\" stroke-width=\"2px\" d=\"M70,92.0 C70,2.0 230.0,2.0 230.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-7dbb82000df8427d8c18d4692c97a364-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,94.0 L62,82.0 78,82.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-7dbb82000df8427d8c18d4692c97a364-0-1\" stroke-width=\"2px\" d=\"M160,92.0 C160,47.0 225.0,47.0 225.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-7dbb82000df8427d8c18d4692c97a364-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M160,94.0 L152,82.0 168,82.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-7dbb82000df8427d8c18d4692c97a364-0-2\" stroke-width=\"2px\" d=\"M340,92.0 C340,47.0 405.0,47.0 405.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-7dbb82000df8427d8c18d4692c97a364-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M340,94.0 L332,82.0 348,82.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-7dbb82000df8427d8c18d4692c97a364-0-3\" stroke-width=\"2px\" d=\"M250,92.0 C250,2.0 410.0,2.0 410.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-7dbb82000df8427d8c18d4692c97a364-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M410.0,94.0 L418.0,82.0 402.0,82.0\" fill=\"currentColor\"/>\n","</g>\n","</svg></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[the]\n","the\n","DET\n","(Do you like the weather?, weather)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"P2dLz0mdaf_h","executionInfo":{"status":"ok","timestamp":1622028486982,"user_tz":-600,"elapsed":262,"user":{"displayName":"darshan Harisamudra shivalingaiah","photoUrl":"","userId":"14589476221181607756"}},"outputId":"a557932e-9d9a-48f6-9d55-1598a585cdd3"},"source":["import spacy\n","spacy.explain(\"ADV\")"],"execution_count":153,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'adverb'"]},"metadata":{"tags":[]},"execution_count":153}]},{"cell_type":"markdown","metadata":{"id":"mgpSmzzwaOiR"},"source":["In the above model am looking for a Noun with previous word has delimiter and Classifing has has commond. In the given examples of the Example is getting wrong output has showen in the above graph. This is the limitation for my model. Further more am also checking Adverbs commands like run faster or swim faster. One more limitation which I checked is Sing a song wont work but Play the song works.\n"]}]}